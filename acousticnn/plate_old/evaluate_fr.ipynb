{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acousticnn.plate.configs.main_dir import main_dir\n",
    "import numpy as np\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "from acousticnn.plate.dataset import get_dataloader\n",
    "from acousticnn.plate.train import evaluate, _evaluate, _generate_preds\n",
    "from acousticnn.plate.train_fsm import evaluate as evaluate_fsm\n",
    "from acousticnn.plate.train_fsm import extract_mean_std, get_mean_from_field_solution\n",
    "from acousticnn.utils.argparser import get_args, get_config\n",
    "from acousticnn.utils.plot import plot_loss\n",
    "import wandb, time, os, torch\n",
    "import matplotlib.pyplot as plt\n",
    "from acousticnn.plate.model import model_factory\n",
    "from matplotlib import rcParams\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "base_path = os.path.join(main_dir, \"experiments/arch\")\n",
    "experiment_path = os.path.join(main_dir, \"experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rcParams['axes.labelsize'] = 12\n",
    "rcParams['axes.titlesize'] = 12\n",
    "rcParams['axes.titlesize'] = 12\n",
    "rcParams[\"figure.figsize\"] = (10 / 2.54, 8 / 2.54)\n",
    "plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(\"color\", plt.cm.Set2([0, 0.5,1]))\n",
    "plt.rcParams['text.usetex'] = False\n",
    "\n",
    "f = np.arange(0, 250)\n",
    "save_dir = \"plots/results\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "model_cfg = \"query_rn18.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(model, conditional):\n",
    "    print(model)\n",
    "    model_cfg = model + \".yaml\"\n",
    "    args = get_args([\"--config\", \"0toy.yaml\", \"--model_cfg\", model_cfg])\n",
    "    return model_factory(**get_config(args.model_cfg), conditional=conditional)\n",
    "\n",
    "def get_results(model, path=None, fsm=False, verbose=False):\n",
    "    net = get_net(model, conditional=config.conditional).cuda()\n",
    "    if path is None:\n",
    "        path = f\"{base_path}/{model}/{difficulty}/checkpoint_best\"\n",
    "    net.load_state_dict(torch.load(path)[\"model_state_dict\"])\n",
    "    prediction, output = _generate_preds(args, config, net, dataloader)\n",
    "    if fsm is False:\n",
    "        results = evaluate(args, config, net, dataloader, report_peak_error=True, epoch=None, report_wasserstein=True, verbose=verbose)\n",
    "    elif fsm is True:\n",
    "        results = evaluate_fsm(args, config, net, dataloader, report_peak_error=True, epoch=None, report_wasserstein=True, verbose=verbose)\n",
    "    results.update({\"prediction\": prediction})\n",
    "    r25, r75 = np.nanquantile(results[\"peak_ratio\"], 0.25), np.nanquantile(results[\"peak_ratio\"], 0.75)\n",
    "    a,b,c = results[\"loss (test/val)\"], results[\"wasserstein\"], results[\"frequency_distance\"]\n",
    "    print(f\"{a:4.2f} & {b:4.2f} & [{r25:4.2f}, {r75:4.2f}] & {c:3.1f}\")\n",
    "    return results\n",
    "\n",
    "def get_field_prediction(batch, dataloader, net):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions, outputs, losses = [], [], []\n",
    "        out_mean, out_std, field_mean, field_std = extract_mean_std(dataloader.dataset)\n",
    "        out_mean, out_std = torch.tensor(out_mean).to(args.device), torch.tensor(out_std).to(args.device)\n",
    "        field_mean, field_std = torch.tensor(field_mean).to(args.device), torch.tensor(field_std).to(args.device)\n",
    "        image, field_solution, output, condition = batch[\"bead_patterns\"], batch[\"z_abs_velocity\"], batch[\"z_vel_mean_sq\"], batch[\"sample_mat\"]\n",
    "        image, field_solution, output, condition = image.to(args.device), field_solution.to(args.device), output.to(args.device), condition.to(args.device)\n",
    "        prediction_field = net(image, condition)\n",
    "        pred_field = prediction_field.clone()\n",
    "        prediction = get_mean_from_field_solution(prediction_field, field_mean, field_std)\n",
    "        prediction.sub_(out_mean).div_(out_std)\n",
    "    return prediction.cpu(), pred_field.cpu()\n",
    "preds = {}\n",
    "preds_field = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    import time\n",
    "    cfgs = os.listdir(os.path.join(main_dir, \"configs/model_cfg/\"))\n",
    "    for model_cfg in cfgs:\n",
    "        print(model_cfg)\n",
    "        args = get_args([\"--config\", \"fsm_V5000.yaml\", \"--model_cfg\", model_cfg])\n",
    "\n",
    "        config = get_config(args.config)\n",
    "        net = model_factory(**get_config(args.model_cfg))\n",
    "        net = net.cuda()\n",
    "        net.eval()\n",
    "        batch = torch.ones((32, 1, 81, 121)).cuda().float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net(batch)\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            net(batch)\n",
    "        end_time = time.time()\n",
    "        print(f\"Forward pass took {end_time - start_time:.6f} seconds.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "difficulty = \"G5000\" # G5000, fsm_V5000\n",
    "args = get_args([\"--config\", f\"{difficulty}.yaml\", \"--model_cfg\", model_cfg])\n",
    "config = get_config(args.config)\n",
    "args.batch_size = 4\n",
    "dataloader = get_dataloader(args, config, logger=None)[2]\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run only for transfer\n",
    "if False:\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    args = get_args([\"--config\", \"G5000.yaml\", \"--model_cfg\", \"fno_conditional.yaml\"])\n",
    "    config_transfer = get_config(args.config)\n",
    "    config_transfer.data_path_ref = config.data_path_ref\n",
    "    trainloader, valloader, testloader, trainset, valset, testset = get_dataloader(args, config_transfer, logger=None, num_workers=0)\n",
    "    dataloader = testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results1 = get_results(\"vit_implicit\", verbose=verbose)\n",
    "results2 = get_results(\"grid_rn18\", verbose=verbose)\n",
    "results3 = get_results(\"query_rn18\", verbose=verbose)\n",
    "results4 = get_results(\"fno_decoder\", verbose=verbose)\n",
    "results5 = get_results(\"deeponet\", verbose=verbose)\n",
    "results6 = get_results(\"query_unet\", verbose=verbose, fsm=True)\n",
    "results7 = get_results(\"fno_fsm\", verbose=verbose, fsm=True)\n",
    "results8 = get_results(\"unet\", verbose=verbose, fsm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"query_unet\" # query_unet\n",
    "difficulty = \"fsm_V5000b\" # G5000, fsm_V5000\n",
    "fsm = True\n",
    "args = get_args([\"--config\", f\"{difficulty}.yaml\", \"--model_cfg\", model_cfg])\n",
    "config = get_config(args.config)\n",
    "args.batch_size = 4\n",
    "dataloader = get_dataloader(args, config, logger=None)[2]\n",
    "\n",
    "#_ = get_results(model, path=os.path.join(experiment_path, f\"transfer/bead_ratio/smaller/{model}/checkpoint_best\"))\n",
    "_ = get_results(model, path=os.path.join(experiment_path, f\"transfer/bead_ratio/smaller/checkpoint_best\"), fsm=fsm)\n",
    "\n",
    "difficulty = \"fsm_V5000a\" # G5000, fsm_V5000\n",
    "args = get_args([\"--config\", f\"{difficulty}.yaml\", \"--model_cfg\", model_cfg])\n",
    "config = get_config(args.config)\n",
    "args.batch_size = 4\n",
    "dataloader = get_dataloader(args, config, logger=None)[2]\n",
    "#_ = get_results(model, path=os.path.join(experiment_path, f\"transfer/bead_ratio/larger/{model}/checkpoint_best\"))\n",
    "_ = get_results(model, path=os.path.join(experiment_path, f\"transfer/bead_ratio/larger/checkpoint_best\"), fsm=fsm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_first = results3.copy()\n",
    "loss_per_sample = np.mean(results3[\"losses_per_f\"], axis=1)\n",
    "print(np.argmin(loss_per_sample), np.argmax(loss_per_sample))\n",
    "print(loss_per_sample[np.argmin(loss_per_sample)], loss_per_sample[np.argmax(loss_per_sample)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_grid= \"RN18 + FNO\"\n",
    "label_query= \"Query-based RN18\"\n",
    "num=11 # 8 and 11 \n",
    "prediction1 = results2[\"prediction\"]\n",
    "prediction2 = results4[\"prediction\"]\n",
    "a = _evaluate(prediction1[num:num+1], output[num:num+1], None, config, args, epoch=0, report_peak_error=True, report_wasserstein=True, dataloader=dataloader)\n",
    "rmse, emd = a[\"loss (test/val)\"], a[\"wasserstein\"]\n",
    "eval_grid = label_grid + \", MSE: \" + f\"{rmse:4.2}\" + \", EMD: \" + f\"{emd:4.3}\"\n",
    "a = _evaluate(prediction2[num:num+1], output[num:num+1], None, config, args, epoch=0, report_peak_error=True, report_wasserstein=True, dataloader=dataloader)\n",
    "rmse, emd = a[\"loss (test/val)\"], a[\"wasserstein\"]\n",
    "eval_query = label_query + \", MSE: \" + f\"{rmse:4.2}\" + \", EMD: \" + f\"{emd:4.3}\"\n",
    "\n",
    "fig,ax = plt.subplots(1, 1, figsize=(10 / 2.54*1.5, 8 / 2.54))\n",
    "ax.plot(f, output[num],  label=\"Reference\", color=\"#909090\", lw=2.5,linestyle='dashed',dashes=[1, 1])\n",
    "ax.plot(f, prediction1[num], alpha = 0.8,  color=\"#e19c2c\", label=eval_grid, lw=2.5)\n",
    "ax.plot(f, prediction2[num], alpha = 0.8, color=\"#55a78c\", label=eval_query, lw=2.5)\n",
    "ax.set_yticks([-4, -2, 0, 2, 4])\n",
    "ax.grid(which=\"major\") \n",
    "ax.set_ylim(-4, 3.5)\n",
    "ax.set_xlabel('frequency')\n",
    "ax.set_ylabel('normalized amplitude')\n",
    "ax.legend(fontsize=11, loc=\"lower left\", frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#plt.savefig(save_dir + f\"prediction_{difficulty}.pdf\", format='pdf', dpi = 600, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_v5000 = results6.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8 / 2.54, 7.5 / 2.54 * 0.9))\n",
    "plot = plot_loss(results_v5000[\"losses_per_f\"], f, ax, quantile=0.5)\n",
    "plot = plot_loss(results6[\"losses_per_f\"], f, ax, quantile=0.5)\n",
    "legend_labels = [\"V-5000\", \"_\", \"G-5000\", \"_\"]\n",
    "ax.legend(legend_labels, fontsize=10, loc='upper left')\n",
    "ax.grid()\n",
    "ax.set_ylim(0, 0.3)\n",
    "ax.set_yticks(np.arange(0, 0.4, 0.1))\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir + \"/mse_over_f.svg\", format='svg', dpi = 600, transparent=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acousticnn.plate.knn.knn_train import AutoEncoder, generate_encoding, get_checker, generate_plots, get_predictions, get_output, pred_fn, get_pred_img, eval_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "setting = \"fsm_V5000\"\n",
    "args = get_args([\"--config\", f\"{setting}.yaml\", \"--model_cfg\", model_cfg])\n",
    "config = get_config(args.config)\n",
    "trainloader, valloader, testloader, trainset, valset, testset= get_dataloader(args, config, logger=None, shuffle=False)\n",
    "dataloader, dataset = valloader, valset\n",
    "k_max=25\n",
    "net = AutoEncoder().cuda()\n",
    "path = os.path.join(base_path, \"knn\", setting, \"checkpoint_best\")\n",
    "net.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference, queries = generate_encoding(trainloader, net), generate_encoding(valloader, net)\n",
    "losses = eval_knn(reference, queries, k_max, config, logger=None, query_set=valset, reference_set=trainset)\n",
    "n_neighbors = np.argmin(losses) + 1\n",
    "print(n_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hard = losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1, 1, figsize=(10 / 2.54*1, 8 / 2.54))\n",
    "ax.plot(losses, label = \"F-2500\", lw=2.5)\n",
    "ax.plot(loss_hard, label = \"V-5000\", lw=2.5)\n",
    "ax.set_yticks(np.arange(0.5, 1.1, 0.2))\n",
    "ax.grid(which=\"major\") \n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.legend(fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir + \"knn_k_sweep.pdf\", format='pdf', dpi = 600, transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 3\n",
    "n_examples = 3\n",
    "prediction = get_pred_img(n_neighbors, trainset, trainloader, dataloader, net).squeeze(2)\n",
    "fig,ax = plt.subplots(n_examples, n_neighbors+ 1, figsize=(10 / 2.54*n_examples, 8 / 2.54*n_neighbors*0.6))\n",
    "[a.axis('off') for a in ax[:,:].flatten()]\n",
    "\n",
    "for i in range(n_examples):\n",
    "    ax[i, 0].imshow(valset[i][\"bead_patterns\"].squeeze(), cmap=plt.cm.gray)\n",
    "    for j in range(n_neighbors):\n",
    "        ax[i, j+1].imshow(prediction[i,j].squeeze(), cmap=plt.cm.gray)\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir + \"knn_nearest_neigbor_images.pdf\", format='pdf', dpi = 600, transparent=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_neighbors = 20\n",
    "use_net=True\n",
    "dataloader, dataset = testloader, testset\n",
    "output = get_output(dataset, config)\n",
    "prediction = pred_fn(n_neighbors, trainset, trainloader, dataloader, net, config, use_net=use_net)\n",
    "results = _evaluate(prediction, output, config=config, args=args, report_peak_error=True, report_wasserstein=True, dataloader=dataloader, epoch=None, logger=None)\n",
    "r25, r75 = np.quantile(results[\"peak_ratio\"], 0.25), np.quantile(results[\"peak_ratio\"], 0.75)\n",
    "a,b,c = results[\"loss (test/val)\"], results[\"wasserstein\"], results[\"frequency_distance\"]\n",
    "print(f\"{a:4.2f} & {b:4.2f} & [{r25:4.2f}, {r75:4.2f}], & {c:3.1f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE over data amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difficulty = \"fsm_V5000\" \n",
    "args = get_args([\"--config\", f\"{difficulty}.yaml\", \"--model_cfg\", \"query_rn18.yaml\"])\n",
    "config = get_config(args.config)\n",
    "dataloader_hard = get_dataloader(args, config, logger=None)[2]\n",
    "dataloader = dataloader_hard\n",
    "experiments = [\"10_percent\", \"25_percent\", \"50_percent\", \"75_percent\"]\n",
    "model = \"query_rn18\"\n",
    "data_vary_path = os.path.join(experiment_path, \"data_variation/\", model, difficulty)\n",
    "paths = [os.path.join(data_vary_path, exp_path,  \"checkpoint_best\") for exp_path in experiments]\n",
    "paths = paths + [os.path.join(experiment_path, f\"arch/{model}/{difficulty}/checkpoint_best\")]\n",
    "[print(path) for path in paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_a = []\n",
    "for path in paths:\n",
    "    results = get_results(model, verbose=verbose, path=path, fsm=False)\n",
    "    loss_a.append(results[\"loss (test/val)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args([\"--config\", f\"{difficulty}.yaml\", \"--model_cfg\", model_cfg])\n",
    "config = get_config(args.config)\n",
    "args.batch_size = 8\n",
    "dataloader_hard = get_dataloader(args, config, logger=None)[2]\n",
    "dataloader = dataloader_hard\n",
    "\n",
    "experiment_paths = [\"10_percent\", \"25_percent\", \"50_percent\", \"75_percent\"]\n",
    "model = \"query_unet\"\n",
    "data_vary_path = os.path.join(experiment_path, \"data_variation/\", model, difficulty)\n",
    "paths = [os.path.join(data_vary_path, exp_path,  \"checkpoint_best\") for exp_path in experiments]\n",
    "paths = paths + [os.path.join(experiment_path, f\"arch/{model}/{difficulty}/checkpoint_best\")]\n",
    "[print(path) for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_b = []\n",
    "for path in paths:\n",
    "    results = get_results(model, verbose=verbose, path=path, fsm=True)\n",
    "    loss_b.append(results[\"loss (test/val)\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse over data amout\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8 / 2.54, 7.5 / 2.54 * 0.9))\n",
    "if difficulty == \"fsm_V5000\":\n",
    "    max_samples = 4500 \n",
    "else:\n",
    "    max_samples = 2000\n",
    "size = np.array([0.1, 0.25, 0.5, 0.75, 1])\n",
    "n_samples = max_samples * size\n",
    "ax.plot(n_samples, loss_a,  'o-', color=\"#b38784\",label=\"Query-RN18\")\n",
    "ax.plot(n_samples, loss_b,  'o-', color=\"#b5b564\", label=\"Query-UNet\")\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(fontsize=10)\n",
    "ax.grid()\n",
    "\n",
    "ax.set_yticks(np.arange(0.2, 0.50, 0.2))\n",
    "ax.set_xticks(np.arange(0, max_samples*1.30, max_samples/2))\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(10)\n",
    "plt.tight_layout()  # Automatically adjusts margins and spacing\n",
    "\n",
    "plt.savefig(save_dir + f\"/data_variation_{difficulty}.svg\", format='svg', dpi = 600, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acoustics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
